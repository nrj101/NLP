{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix seeds to control for randomly varying outputs (due to selection of equally-frequent words in a random fashion)\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "\n",
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import heapq\n",
    "from utils import *         # helper functions like get_word2count, get_word2idx_and_idx2word,  etc.  \n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences:  57340\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Stars for marriage',\n",
       " 'During the first pass of Phase 3,, references to the actual addresses of index words and electronic switches are collected and the availability table is updated.',\n",
       " 'As we expected, on the following day my Uncle was completely recovered and opened the store as usual at 10 in the morning.',\n",
       " 'Books are not the only resource of the system.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "from nltk.tokenize.moses import MosesDetokenizer\n",
    "\n",
    "# Use a detokenizer to remove the annotations/special tokens/tags from the brown corpus\n",
    "mdetok = MosesDetokenizer()\n",
    "\n",
    "dataset = [mdetok.detokenize(' '.join(sent).replace('``', '\"').replace(\"''\", '\"').replace('`', \"'\").split(), return_str=True)  for sent in brown.sents()]\n",
    "print('Total sentences: ',len(dataset))\n",
    "random.shuffle(dataset)\n",
    "dataset[0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He had just paid a brief visit to the Frick Collection to admire his favorite paintings by Rembrandt and Franz Hals.\n",
      "he had just paid a brief visit to the frick collection to admire his favorite paintings by rembrandt and franz hals.\n"
     ]
    }
   ],
   "source": [
    "print(dataset[1450])\n",
    "sentences = preprocess(dataset)[:10000] # keep first 10k sentences only to allow for faster run-time.\n",
    "print(sentences[1450])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count word occurences & prepare histogram of top-10000 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19181\n"
     ]
    }
   ],
   "source": [
    "# Histogram of occurences of words in the dataset\n",
    "word2count = get_word2count(sentences)\n",
    "print(len(word2count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of words selected:  5000 \n",
      "\n",
      "the   :   12251 \n",
      ".   :   10000 \n",
      "of   :   6301 \n",
      "mosque   :   4 \n",
      "purdew   :   4 \n",
      "simms   :   4 \n"
     ]
    }
   ],
   "source": [
    "V = 10000                                     # No. of words in vocabulary, i.e. vocabulary size\n",
    "\n",
    "freq_words = heapq.nlargest(V, word2count, key=word2count.get)  # Select only \"V\" most frequent words\n",
    "print('# of words selected: ',V,'\\n')\n",
    "\n",
    "# Print first 3 and last 3 words with their occurence rate\n",
    "for i in range(3):\n",
    "    print('{}   :   {} '.format(freq_words[i],word2count[freq_words[i]]))  \n",
    "for i in range(3):\n",
    "    print('{}   :   {} '.format(freq_words[-i-1],word2count[freq_words[-i-1]]))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map each word to a unique number (index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of words in vocabulary: 5003\n",
      "['determination', 'knife', 'hardly', 'glance', 'herd', 'satisfy']\n",
      "935 2247 649 987 2498 4694\n",
      "determination knife hardly glance herd satisfy\n"
     ]
    }
   ],
   "source": [
    "# Mapping most frequent words in the dataset to a unique index. This is used to translate sentences consisting of words to a list of numbers (indexes).\n",
    "word2idx, idx2word = get_word2idx_and_idx2word(sentences, freq_words)\n",
    "\n",
    "# Update V variable to include any extra tokens like [\"START\",\"END\",\"UNKNOWN\"] added in word2idx\n",
    "V = len(idx2word)\n",
    "\n",
    "print('No. of words in vocabulary: {}'.format(V))\n",
    "\n",
    "# Print a few examples of mappings\n",
    "sample_words = random.sample(freq_words, 6)\n",
    "print(sample_words)\n",
    "\n",
    "print(word2idx[sample_words[0]], word2idx[sample_words[1]], word2idx[sample_words[2]], word2idx[sample_words[3]],word2idx[sample_words[4]],word2idx[sample_words[5]])\n",
    "\n",
    "# Verify that the mappings are correct\n",
    "print(idx2word[word2idx[sample_words[0]]], idx2word[word2idx[sample_words[1]]], idx2word[word2idx[sample_words[2]]], idx2word[word2idx[sample_words[3]]],idx2word[word2idx[sample_words[4]]],idx2word[word2idx[sample_words[5]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform all the sentences using these {word:index} mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  he had just paid a brief visit to the frick collection to admire his favorite paintings by rembrandt and franz hals.\n",
      "Transformed:  [0, 99, 75, 341, 659, 85, 1412, 1998, 12, 7, 5002, 1024, 12, 5002, 101, 3637, 2458, 188, 5002, 16, 5002, 5002, 5, 1]\n",
      "['START', 'he', 'had', 'just', 'paid', 'a', 'brief', 'visit', 'to', 'the', 'UNKNOWN', 'collection', 'to', 'UNKNOWN', 'his', 'favorite', 'paintings', 'by', 'UNKNOWN', 'and', 'UNKNOWN', 'UNKNOWN', '.', 'END']\n"
     ]
    }
   ],
   "source": [
    "new_sentences = transform_sentences(sentences, word2idx)\n",
    "\n",
    "# Print the same sentence from the \"Preprocessing\" part in new form\n",
    "print('Original: ',sentences[1450])\n",
    "print('Transformed: ',new_sentences[1450])\n",
    "print([idx2word[idx] for idx in new_sentences[1450]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Counting method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting method bigram-model\n",
    "def get_bigram_probs(sentences, word2idx, vocab_size):\n",
    "    \"\"\"Convert each sentence in \"sentences\" from a collection of words to a collection of numbers, where a number is the unique index the original word was mapped to.\n",
    "    Arguments:\n",
    "    sentences -- list, length N, each element containing numerical format of corresponding sentence in the dataset.\n",
    "    word2idx -- dictionary, with {word: mapped_index} as key:value pairs.\n",
    "    vocab_size -- int, number of words in the vocabulary\n",
    "    Returns:\n",
    "    bigram_probs -- numpy.ndarray, shape-(vocab_size, vocab_size), contains the probability distributions for next word (along axis 1, i.e. columns) given current word (along axis 0, i.e. rows).\"\"\"\n",
    "    \n",
    "    # not initializing with zeros so that there is some minimalistic probability for all bigrams.\n",
    "    bigram_matrix = np.ones((vocab_size, vocab_size))  \n",
    "    \n",
    "    for sentence in sentences:\n",
    "        for i in range(1, len(sentence)):\n",
    "            bigram_matrix[sentence[i-1], sentence[i]] += 1\n",
    "    bigram_matrix /= bigram_matrix.sum(axis=1, keepdims=True)\n",
    "    return bigram_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learn probability distribution of bi-grams in the \"new_sentences\" dataset using above function\n",
    "bigram_probs = get_bigram_probs(new_sentences, word2idx, vocab_size=V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1.\n",
      "Sentence: the UNKNOWN data given include UNKNOWN UNKNOWN and unit cell dimensions .         Score: -5.1772\n",
      "Sentence: this is one of the most exciting events people choose to go to .         Score: -5.2647\n",
      "Sentence: church kind like i about care interesting suddenly         Score: -7.6008\n",
      "Sentence: church kind like i about care interesting suddenly .         Score: -6.8837\n",
      "\n",
      "\n",
      "Task 2.\n",
      "Law of the state constitution tile goods projects control powder station trust montero foster woods monday swung regional miss plato new thinks cultures auto\n",
      "High tax lodge prime built composed scientists trade vote morris fifteenth levels directly cultures collective exchange mirror escort accordingly holder herbert installed\n",
      "There were reports red hundred chapter scene conflict toes whispered propose aside symbolic chaos nurse intent information promises mental anyway march louis turn\n"
     ]
    }
   ],
   "source": [
    "# 1. Sentence scores: the log-probability (normalized by length of sentence) of a sentence being picked among the universe of sentences possible using top-1500 words of the dataset.\n",
    "print('Task 1.')\n",
    "# Test on a sentence from the dataset\n",
    "print(\"Sentence: {}         Score: {}\".format(inv_transform_sentence(new_sentences[34], idx2word), '%.4f'%get_sentence_score(new_sentences[34], bigram_probs)) )\n",
    "\n",
    "# Test on a correct new sentence\n",
    "test_sentence = transform_sentences([\"This is one of the most exciting events people choose to go to.\"], word2idx)\n",
    "print(\"Sentence: {}         Score: {}\".format(inv_transform_sentence(test_sentence[0], idx2word), '%.4f'%get_sentence_score(test_sentence[0], bigram_probs)) )\n",
    "\n",
    "# Test on an incorrect new sentence\n",
    "test_sentence = transform_sentences([\"Church kind like I about care interesting suddenly\"], word2idx)\n",
    "print(\"Sentence: {}         Score: {}\".format(inv_transform_sentence(test_sentence[0], idx2word), '%.4f'%get_sentence_score(test_sentence[0], bigram_probs)) )\n",
    "\n",
    "# Add the .(\"dot\") at the end of the above sentence to see the score improving a bit.\n",
    "test_sentence = transform_sentences([\"Church kind like I about care interesting suddenly.\"], word2idx)\n",
    "print(\"Sentence: {}         Score: {}\\n\\n\".format(inv_transform_sentence(test_sentence[0], idx2word), '%.4f'%get_sentence_score(test_sentence[0], bigram_probs)) )\n",
    "\n",
    "\n",
    "# 2. Sentence completion: completion of a sentence given some initial words using the probability distribution learned by the counting-method bigram model\n",
    "print('Task 2.')\n",
    "print(generate_sentence(\"Law of the state\", bigram_probs, word2idx, idx2word, max_len=20) )\n",
    "print(generate_sentence(\"High tax\", bigram_probs, word2idx, idx2word, max_len=20) )\n",
    "print(generate_sentence(\"There were reports\", bigram_probs, word2idx, idx2word, max_len=20) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Simple Neural Network\n",
    "##### (Only Softmax layer, no hidden layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 complete!!   time_elapsed: 2836.61s-   Loss: 6.0573\n",
      "Training complete!!   time_elapsed: 2836.61s\n"
     ]
    }
   ],
   "source": [
    "W = np.random.uniform(size=(V,V))        # Initialize the weights of the network\n",
    "\n",
    "# Specify Hyperparameters for the network\n",
    "epochs = 1\n",
    "lr = 0.01\n",
    "\n",
    "loss_hist = []\n",
    "training_start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    for i, sentence in enumerate(new_sentences):\n",
    "        n = len(sentence)\n",
    "        \n",
    "        # convert each word of the sentence to one-hot-encoded format\n",
    "        inp_sentence = np.zeros((n, V))\n",
    "        inp_sentence[np.arange(n), sentence] = 1  \n",
    "        \n",
    "        X = inp_sentence[:n-1, :]              # shape: N x V\n",
    "        y = inp_sentence[1:, :].T              # shape: V x N\n",
    "        \n",
    "        preds = softmax(np.dot(W, X.T))    # 1. Forward propagation     shape: V x N\n",
    "        grads = np.dot((preds - y), X)     # 2. Compute gradients       shape: V x N\n",
    "        \n",
    "        W = W - lr* grads                  # 3. Gradient descent step\n",
    "        \n",
    "        loss = - np.sum(y*np.log(preds), axis=None)/n-1\n",
    "        loss_hist.append(loss)\n",
    "        \n",
    "        if i%50==49:\n",
    "            print('Epoch: {}/{}   --   Sentence: {}/{}   --   Loss: {}'.format(epoch+1, epochs, '%5.0f'%(i+1), '%5.0f'%len(new_sentences), '%.4f'%loss), end='\\r')\n",
    "    print('Epoch {} complete!!   time_elapsed: {}s'.format(epoch+1, '%.2f'%(time.time()-epoch_start_time)))\n",
    "\n",
    "print('Training complete!!   time_elapsed: {}s'.format('%.2f'%(time.time()-training_start_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1.\n",
      "Sentence: the UNKNOWN data given include UNKNOWN UNKNOWN and unit cell dimensions .         Score: -6.6371\n",
      "Sentence: this is one of the most exciting events people choose to go to .         Score: -8.0741\n",
      "Sentence: church kind like i about care interesting suddenly         Score: -7.8909\n",
      "Sentence: church kind like i about care interesting suddenly .         Score: -7.9584\n",
      "\n",
      "\n",
      "Task 2.\n",
      "Law of the state does early identified national inside elbow rare anonymous activation efforts spare cathy ham pull earnestly before conclusion convinced employees men\n",
      "High tax bureau control their fence workers later floors ruth missing banks fun freezing drunk sat fair planning guarantee editors patients we\n",
      "There were reports fromm attitude principle site board attributed rhode frames mouth methodist laboratory prize ignore equipment leadership negro print consumer proposals grinned\n"
     ]
    }
   ],
   "source": [
    "# 1. Sentence scores: the log-probability (normalized by length of sentence) of a sentence being picked among the universe of sentences possible using top-1500 words of the dataset.\n",
    "print('Task 1.')\n",
    "# Test on a sentence from the dataset\n",
    "print(\"Sentence: {}         Score: {}\".format(inv_transform_sentence(new_sentences[34], idx2word), '%.4f'%get_sentence_score_nn1(new_sentences[34], W)) )\n",
    "\n",
    "# Test on a correct new sentence\n",
    "test_sentence = transform_sentences([\"This is one of the most exciting events people choose to go to.\"], word2idx)\n",
    "print(\"Sentence: {}         Score: {}\".format(inv_transform_sentence(test_sentence[0], idx2word), '%.4f'%get_sentence_score_nn1(test_sentence[0], W)) )\n",
    "\n",
    "# Test on an incorrect new sentence\n",
    "test_sentence = transform_sentences([\"Church kind like I about care interesting suddenly\"], word2idx)\n",
    "print(\"Sentence: {}         Score: {}\".format(inv_transform_sentence(test_sentence[0], idx2word), '%.4f'%get_sentence_score_nn1(test_sentence[0], W)) )\n",
    "\n",
    "# Add the .(\"dot\") at the end of the above sentence to see the score improving a bit.\n",
    "test_sentence = transform_sentences([\"Church kind like I about care interesting suddenly.\"], word2idx)\n",
    "print(\"Sentence: {}         Score: {}\\n\\n\".format(inv_transform_sentence(test_sentence[0], idx2word), '%.4f'%get_sentence_score_nn1(test_sentence[0], W)) )\n",
    "\n",
    "\n",
    "# 2. Sentence completion: completion of a sentence given some initial words using the probability distribution learned by the counting-method bigram model\n",
    "print('Task 2.')\n",
    "print(generate_sentence_nn1(\"Law of the state\",   W, word2idx, idx2word, max_len=20) )\n",
    "print(generate_sentence_nn1(\"High tax\",           W, word2idx, idx2word, max_len=20) )\n",
    "print(generate_sentence_nn1(\"There were reports\", W, word2idx, idx2word, max_len=20) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Neural Network with 1 hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 complete!!   time_elapsed: 206.16s--   Loss: 4.9309\n",
      "Epoch 2 complete!!   time_elapsed: 204.17s--   Loss: 4.7516\n",
      "Epoch 3 complete!!   time_elapsed: 205.16s--   Loss: 4.6671\n",
      "Epoch 4 complete!!   time_elapsed: 210.69s--   Loss: 4.6085\n",
      "Epoch 5 complete!!   time_elapsed: 211.47s--   Loss: 4.5597\n",
      "Training complete!!   time_elapsed: 1037.65s\n"
     ]
    }
   ],
   "source": [
    "# Specify Hyperparameters for the network\n",
    "D = 300                                     # hidden layer size\n",
    "epochs = 5\n",
    "lr = 0.001\n",
    "\n",
    "W1 = np.random.uniform(size=(D, V))        # Initialize the weights of the network\n",
    "W2 = np.random.uniform(size=(V, D))        # Initialize the weights of the network\n",
    "\n",
    "\n",
    "loss_hist = []\n",
    "training_start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    for i, sentence in enumerate(new_sentences):\n",
    "        n = len(sentence)\n",
    "        \n",
    "        # convert each word of the sentence to one-hot-encoded format\n",
    "        inp_sentence = np.zeros((n, V))\n",
    "        inp_sentence[np.arange(n), sentence] = 1  \n",
    "        \n",
    "        X = inp_sentence[:n-1, :].T            # shape: V x N\n",
    "        y = inp_sentence[1:, :].T              # shape: V x N\n",
    "        \n",
    "        # 1. Forward propagation\n",
    "        z1 = np.dot(W1, X)                     # shape: D x N\n",
    "        a1 = z1 * (z1>0)                       # shape: D x N\n",
    "        \n",
    "        z2 = np.dot(W2, a1)                    # shape: V x N\n",
    "        preds = softmax(z2)                    # shape: V x N\n",
    "        \n",
    "        # 2. Loss calculation\n",
    "        loss = - np.sum(y*np.log(preds), axis=None)/n-1   # float, single number\n",
    "        \n",
    "        # 3. Back propagation\n",
    "        grads_z2 = preds - y                   # shape: V x N\n",
    "        grads_W2 = np.dot(grads_z2, a1.T)      # shape: V x D\n",
    "        \n",
    "        grads_a1 = np.dot(W2.T, grads_z2)      # shape: D x N\n",
    "        grads_z1 = grads_a1 * (a1>0)           # shape: D x N\n",
    "        grads_W1 = np.dot(grads_z1, X.T)       # shape: D x V\n",
    "        \n",
    "        # 4. Update weights\n",
    "        W2 = W2 - lr* grads_W2 \n",
    "        W1 = W1 - lr* grads_W1 \n",
    "        \n",
    "        loss_hist.append(loss)\n",
    "        \n",
    "        if i%50==49:\n",
    "            print('Epoch: {}/{}   --   Sentence: {}/{}   --   Loss: {}'.format(epoch+1, epochs, '%5.0f'%(i+1), '%5.0f'%len(new_sentences), '%.4f'%loss), end='\\r')\n",
    "    print('Epoch {} complete!!   time_elapsed: {}s'.format(epoch+1, '%.2f'%(time.time()-epoch_start_time)))\n",
    "\n",
    "print('Training complete!!   time_elapsed: {}s'.format('%.2f'%(time.time()-training_start_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1.\n",
      "Sentence: the UNKNOWN data given include UNKNOWN UNKNOWN and unit cell dimensions .         Score: -7.6060\n",
      "Sentence: this is one of the most exciting events people choose to go to .         Score: -7.6519\n",
      "Sentence: church kind like i about care interesting suddenly         Score: -8.4152\n",
      "Sentence: church kind like i about care interesting suddenly .         Score: -8.5742\n",
      "\n",
      "\n",
      "Task 2.\n",
      "Law of the state was in introduction by is UNKNOWN has UNKNOWN you member he and mock af the af and more signal and\n",
      "High tax of and avoid and for taken and of him of jazz of he would visit action the why of far\n",
      "There were reports of from . will for UNKNOWN . eliminated more without something . us UNKNOWN . for as by to immediate\n"
     ]
    }
   ],
   "source": [
    "# 1. Sentence scores: the log-probability (normalized by length of sentence) of a sentence being picked among the universe of sentences possible using top-1500 words of the dataset.\n",
    "print('Task 1.')\n",
    "# Test on a sentence from the dataset\n",
    "print(\"Sentence: {}         Score: {}\".format(inv_transform_sentence(new_sentences[34], idx2word), '%.4f'%get_sentence_score_nn2(new_sentences[34], W1, W2)) )\n",
    "\n",
    "# Test on a correct new sentence\n",
    "test_sentence = transform_sentences([\"This is one of the most exciting events people choose to go to.\"], word2idx)\n",
    "print(\"Sentence: {}         Score: {}\".format(inv_transform_sentence(test_sentence[0], idx2word), '%.4f'%get_sentence_score_nn2(test_sentence[0], W1, W2)) )\n",
    "\n",
    "# Test on an incorrect new sentence\n",
    "test_sentence = transform_sentences([\"Church kind like I about care interesting suddenly\"], word2idx)\n",
    "print(\"Sentence: {}         Score: {}\".format(inv_transform_sentence(test_sentence[0], idx2word), '%.4f'%get_sentence_score_nn2(test_sentence[0], W1, W2)) )\n",
    "\n",
    "# Add the .(\"dot\") at the end of the above sentence to see the score improving a bit.\n",
    "test_sentence = transform_sentences([\"Church kind like I about care interesting suddenly.\"], word2idx)\n",
    "print(\"Sentence: {}         Score: {}\\n\\n\".format(inv_transform_sentence(test_sentence[0], idx2word), '%.4f'%get_sentence_score_nn2(test_sentence[0], W1, W2)) )\n",
    "\n",
    "\n",
    "# 2. Sentence completion: completion of a sentence given some initial words using the probability distribution learned by the counting-method bigram model\n",
    "print('Task 2.')\n",
    "print(generate_sentence_nn2(\"Law of the state\",   W1, W2, word2idx, idx2word, max_len=20) )\n",
    "print(generate_sentence_nn2(\"High tax\",           W1, W2, word2idx, idx2word, max_len=20) )\n",
    "print(generate_sentence_nn2(\"There were reports\", W1, W2, word2idx, idx2word, max_len=20) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
